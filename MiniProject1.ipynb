{"cells":[{"cell_type":"code","source":["!pip install transformers\n","!pip install light-the-torch\n","!ltt install torch torchvision\n","!pip install fastai --upgrade"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"o6GRTmGTaGbq","executionInfo":{"status":"ok","timestamp":1698116714951,"user_tz":240,"elapsed":25918,"user":{"displayName":"Nat Swinger","userId":"02225308103184052469"}},"outputId":"ad3e4d29-6462-456d-949f-0b26b1ae992e"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting transformers\n","  Downloading transformers-4.34.1-py3-none-any.whl (7.7 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m51.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.4)\n","Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n","  Downloading huggingface_hub-0.18.0-py3-none-any.whl (301 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.23.5)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.6.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n","Collecting tokenizers<0.15,>=0.14 (from transformers)\n","  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m109.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n","  Downloading safetensors-0.4.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m84.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.1)\n","Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (2023.6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.16.4->transformers) (4.5.0)\n","Collecting huggingface-hub<1.0,>=0.16.4 (from transformers)\n","  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m33.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.7.22)\n","Installing collected packages: safetensors, huggingface-hub, tokenizers, transformers\n","Successfully installed huggingface-hub-0.17.3 safetensors-0.4.0 tokenizers-0.14.1 transformers-4.34.1\n","Collecting light-the-torch\n","  Downloading light_the_torch-0.7.5-py3-none-any.whl (14 kB)\n","Requirement already satisfied: pip<23.3,>=22.3 in /usr/local/lib/python3.10/dist-packages (from light-the-torch) (23.1.2)\n","Installing collected packages: light-the-torch\n","Successfully installed light-the-torch-0.7.5\n","Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n","Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu118)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.12.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n","Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.7.22)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n","Requirement already satisfied: fastai in /usr/local/lib/python3.10/dist-packages (2.7.13)\n","Requirement already satisfied: pip in /usr/local/lib/python3.10/dist-packages (from fastai) (23.1.2)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from fastai) (23.2)\n","Requirement already satisfied: fastdownload<2,>=0.0.5 in /usr/local/lib/python3.10/dist-packages (from fastai) (0.0.7)\n","Requirement already satisfied: fastcore<1.6,>=1.5.29 in /usr/local/lib/python3.10/dist-packages (from fastai) (1.5.29)\n","Requirement already satisfied: torchvision>=0.11 in /usr/local/lib/python3.10/dist-packages (from fastai) (0.16.0+cu118)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from fastai) (3.7.1)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from fastai) (1.5.3)\n","Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from fastai) (2.31.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from fastai) (6.0.1)\n","Requirement already satisfied: fastprogress>=0.2.4 in /usr/local/lib/python3.10/dist-packages (from fastai) (1.0.3)\n","Requirement already satisfied: pillow>=9.0.0 in /usr/local/lib/python3.10/dist-packages (from fastai) (9.4.0)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (from fastai) (1.2.2)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from fastai) (1.11.3)\n","Requirement already satisfied: spacy<4 in /usr/local/lib/python3.10/dist-packages (from fastai) (3.6.1)\n","Requirement already satisfied: torch<2.2,>=1.10 in /usr/local/lib/python3.10/dist-packages (from fastai) (2.1.0+cu118)\n","Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (3.0.12)\n","Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (1.0.5)\n","Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (1.0.10)\n","Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (2.0.8)\n","Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (3.0.9)\n","Requirement already satisfied: thinc<8.2.0,>=8.1.8 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (8.1.12)\n","Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (1.1.2)\n","Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (2.4.8)\n","Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (2.0.10)\n","Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (0.9.0)\n","Requirement already satisfied: pathy>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (0.10.2)\n","Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (6.4.0)\n","Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (4.66.1)\n","Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (1.23.5)\n","Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (1.10.13)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (3.1.2)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (67.7.2)\n","Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<4->fastai) (3.3.0)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->fastai) (3.3.0)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->fastai) (3.4)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->fastai) (2.0.7)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->fastai) (2023.7.22)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=1.10->fastai) (3.12.4)\n","Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=1.10->fastai) (4.5.0)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=1.10->fastai) (1.12)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=1.10->fastai) (3.2)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=1.10->fastai) (2023.6.0)\n","Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch<2.2,>=1.10->fastai) (2.1.0)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fastai) (1.1.1)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fastai) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fastai) (4.43.1)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fastai) (1.4.5)\n","Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fastai) (3.1.1)\n","Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->fastai) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->fastai) (2023.3.post1)\n","Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fastai) (1.3.2)\n","Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn->fastai) (3.2.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib->fastai) (1.16.0)\n","Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4->fastai) (0.7.11)\n","Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.2.0,>=8.1.8->spacy<4->fastai) (0.1.3)\n","Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<4->fastai) (8.1.7)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<4->fastai) (2.1.3)\n","Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch<2.2,>=1.10->fastai) (1.3.0)\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"id":"zjQ-QQc1T1gv","executionInfo":{"status":"ok","timestamp":1698116721450,"user_tz":240,"elapsed":6503,"user":{"displayName":"Nat Swinger","userId":"02225308103184052469"}}},"outputs":[],"source":["from transformers import AutoModelForMaskedLM, AutoTokenizer\n","from os import listdir\n","from os.path import isfile, join\n","import pandas as pd\n","import torch\n","import numpy as np\n","from scipy.stats import sem, norm\n","from sklearn.decomposition import PCA\n","from sklearn.model_selection import train_test_split\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.neural_network import MLPClassifier\n","from sklearn.metrics import f1_score\n","import matplotlib.pyplot as plt\n","from datetime import datetime\n","import time\n","from collections import OrderedDict"]},{"cell_type":"markdown","metadata":{"id":"Mknsxdg1Umf9"},"source":["# Util Functions"]},{"cell_type":"code","source":["def get_subreddits(v):\n","    condition_subreddits = ['EDAnonymous',\n","                            'addiction',\n","                            'adhd',\n","                            'alcoholism',\n","                            'anxiety',\n","                            'autism',\n","                            'bipolarreddit',\n","                            'bpd',\n","                            'depression',\n","                            'ptsd',\n","                            'schizophrenia']\n","\n","    mh_subreddits = ['COVID19_support',\n","                     'EDAnonymous',\n","                     'addiction',\n","                     'adhd',\n","                     'alcoholism',\n","                     'anxiety',\n","                     'autism',\n","                     'bipolarreddit',\n","                     'bpd',\n","                     'depression',\n","                     'healthanxiety'\n","                     'lonely',\n","                     'mental_health',\n","                     'ptsd',\n","                     'schizophrenia',\n","                     'socialanxiety',\n","                     'suicide watch']\n","\n","    non_mh_subreddits = ['conspiracy',\n","                         'divorce',\n","                         'fitness',\n","                         'guns',\n","                         'jokes',\n","                         'legaladvice',\n","                         'meditation',\n","                         'parenting',\n","                         'personalfinance',\n","                         'relationships'\n","                         'teaching']\n","\n","    full_reddit = mh_subreddits + non_mh_subreddits\n","\n","    if v == 'c':\n","        return condition_subreddits\n","    if v == 'mh':\n","        return mh_subreddits\n","    if v == 'nmh':\n","        return non_mh_subreddits\n","    else:\n","        return full_reddit\n","\n","\n","def load_model(global_file_path):\n","    try:\n","        pytorch_lm = torch.load(join(global_file_path, 'model.pth'))\n","    except FileNotFoundError:\n","        pytorch_lm = AutoModelForMaskedLM.from_pretrained('mnaylor/psychbert-cased', from_flax=True,\n","                                                          output_hidden_states=True)\n","        torch.save(pytorch_lm, join(global_file_path, 'model.pth'))\n","    pytorch_tk = AutoTokenizer.from_pretrained(\"mnaylor/psychbert-cased\")\n","    return pytorch_lm, pytorch_tk\n","\n","\n","def get_max_subsample_size(path, fs):\n","    min_len = 1000000\n","    for f in fs:\n","        df = clean(pd.read_csv(join(path, f), engine='python', on_bad_lines='skip'))\n","        if len(df) < min_len:\n","            min_len = len(df)\n","    return min_len\n","\n","\n","def load_data(global_file_path, file_name=None, subsample=True, subsample_size=10, subreddit='c', clean_data=True):\n","    # subsamples balanced across subreddits with subsample_size type int, unbalanced with float\n","    path = join(global_file_path, 'reddit_mental_health_dataset_nswinger')\n","    sub_names = {'c': 'condition', 'mh': 'mental_health', 'nmh': 'non_mental_health', 'all': 'full_reddit'}\n","\n","    if subsample:\n","        save_file = join(global_file_path, f'{sub_names[subreddit]}_df_{subsample_size}.pkl')\n","    else:\n","        save_file = join(global_file_path, f'{sub_names[subreddit]}_df.pkl')\n","\n","    if file_name is None:\n","        try:\n","            df = pd.read_pickle(save_file)\n","        except FileNotFoundError:\n","            all_files = [f for f in listdir(path) if isfile(join(path, f)) and any(n == f.split('_')[0] for n in get_subreddits(subreddit))]\n","\n","            if subsample:\n","                max_subsample_size = get_max_subsample_size(path, all_files)\n","                if subsample_size >= max_subsample_size:\n","                    print(f'Warning: Selected subsample size is too large, continuing with subsamples of size {max_subsample_size}...\\n')\n","                    subsample_size = max_subsample_size\n","                    save_file = join(global_file_path, f'{sub_names[subreddit]}_df_{subsample_size}.pkl')\n","\n","            all_files.sort()\n","            df = pd.DataFrame()\n","            for f in all_files:\n","                df2 = pd.read_csv(join(path, f), engine='python', on_bad_lines='skip')\n","                if clean_data:\n","                    df2 = clean(df2)\n","                if subsample:\n","                    df2 = subsample_df(df2, subsample_size)\n","                df = pd.concat([df, df2])\n","            if 'covid19_total' in df.columns:\n","                df = df.drop(columns=['covid19_total'])\n","            df.to_pickle(save_file)\n","    else:\n","        df = pd.read_csv(join(path, file_name), engine='python', on_bad_lines='skip')\n","        if clean_data:\n","            df = clean(df)\n","        if subsample:\n","            max_subsample_size = get_max_subsample_size(path, file_name)\n","            if subsample_size >= max_subsample_size:\n","                print(f'Warning: Selected subsample size is too large, continuing with subsamples of size {max_subsample_size}...\\n')\n","                subsample_size = max_subsample_size\n","        df = subsample_df(df, subsample_size)\n","\n","    return df\n","\n","\n","\n","def subsample_df(df, subsample):\n","    if type(subsample) == float:\n","        subsample = int(df.shape[0] * subsample)\n","    df = df.reset_index(drop=True)\n","    df2 = df.loc[np.random.choice(df.index, subsample, replace=False)]\n","    return df2\n","\n","\n","def clean(df):\n","    # remove author duplicates and shuffle so we dont keep only first posts in time\n","    reddit_data = df.sample(frac=1)  # shuffle\n","    reddit_data = reddit_data.drop_duplicates(subset='author', keep='first')\n","    reddit_data = reddit_data[\n","        ~reddit_data.author.str.contains('|'.join(['bot', 'BOT', 'Bot']))]  # There is at least one bot per subreddit\n","    reddit_data = reddit_data[\n","        ~reddit_data.post.str.contains('|'.join(['quote', 'QUOTE', 'Quote']))]  # Remove posts in case quotes are long\n","    reddit_data = reddit_data.reset_index(drop=True)\n","    return reddit_data\n","\n","\n","def tokenize(global_file_path, tokenizer, model, posts, max_batch_size, max_tokens, subreddit):\n","    device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n","\n","    num_batches = int(np.ceil(len(posts) / max_batch_size))\n","    post_batches = np.array_split(posts, num_batches)\n","\n","    model.eval()\n","    model.to(device)\n","\n","    embeddings = []\n","\n","    print(f'Tokenizing posts and feeding to pretrained BERT model...\\n')\n","    start = time.time()\n","    for i, post_batch in enumerate(post_batches):\n","        encoded_input = tokenizer(post_batch.tolist(), padding=True, truncation=True, max_length=max_tokens, return_tensors=\"pt\")\n","\n","        # Move tokenized data to GPU just before forwarding through the model\n","        encoded_input = {k: v.to(device) for k, v in encoded_input.items()}\n","\n","        with torch.no_grad():\n","            output = model(**encoded_input)\n","            hidden_states = output.hidden_states\n","\n","            token_vecs = hidden_states[-2]\n","            sentence_embedding = torch.mean(token_vecs, dim=1)\n","            embeddings.extend(sentence_embedding.cpu().numpy())\n","\n","    stop = time.time()\n","    print(f'BERT embeddings generated, run time: {stop-start:.2f} s...\\n   embedding set has shape [{len(embeddings)} x {len(embeddings[0])}]\\n')\n","    np.save(join(global_file_path, f'BERT_embeddings-{subreddit}-mb{max_batch_size}-mt{max_tokens}-{len(embeddings)}x{len(embeddings[0])}'), embeddings)\n","\n","    return embeddings\n","\n","\n","def get_embedding(sentence, posts, embeddings):\n","    index = posts.index(sentence)\n","    return embeddings[index]\n","\n","\n","def load_BERT_embeddings(global_file_path, posts, max_batch_size=5, max_tokens=512, subreddit='c'):\n","    try:\n","        start = time.time()\n","        embeddings = np.load(join(global_file_path, f'BERT_embeddings-{subreddit}-mb{max_batch_size}-mt{max_tokens}-{len(posts)}x768.npy'))\n","        stop = time.time()\n","        print(f'Pretrained BERT embeddings loaded, run time: {stop-start:.2f} s...\\n   embedding set has shape [{len(embeddings)} x {len(embeddings[0])}]\\n')\n","    except FileNotFoundError:\n","        print(f'Embedding save file not found, generating from scratch...\\n')\n","\n","        print(f'Loading BERT model and tokenizer...\\n')\n","        start = time.time()\n","        psychBERT_model, tokenizer = load_model(global_file_path)\n","        stop = time.time()\n","        print(f'BERT model and tokenizer loaded, run time: {stop-start:.2f} s...\\n')\n","\n","        embeddings = tokenize(global_file_path, tokenizer, psychBERT_model, posts, max_batch_size, max_tokens, subreddit)\n","\n","    return embeddings\n","\n","\n","def classify(global_file_path, X, y, n_trials=50, perform_pca=False, n_pca_components=150, n_eig_view=20, vec_name='', pca_only=False):\n","    if pca_only:\n","        perform_pca = True\n","\n","    f1s = []\n","    accs = []\n","\n","    if perform_pca:\n","        print(f'Performing PCA on {vec_name} embeddings...')\n","        evrs, evs = [], []\n","    for n in range(n_trials):\n","        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)\n","\n","        sc = StandardScaler()\n","        X_train = sc.fit_transform(X_train)\n","        X_test = sc.transform(X_test)\n","\n","        if perform_pca:\n","            if n_pca_components is not None and n_pca_components >= len(X_train[0]):\n","                n_pca_components = None\n","\n","            pca = PCA(n_components=n_pca_components)\n","\n","            X_train = pca.fit_transform(X_train)\n","            X_test = pca.transform(X_test)\n","\n","            explained_variance_ratio = pca.explained_variance_ratio_\n","            explained_variance = pca.explained_variance_\n","            evrs.append(explained_variance_ratio)\n","            evs.append(explained_variance)\n","\n","        if not pca_only:\n","            clf = MLPClassifier(random_state=1, max_iter=300).fit(X_train, y_train)\n","            y_pred = clf.predict(X_test)\n","            accuracy = clf.score(X_test, y_test)\n","            f1 = f1_score(y_test, y_pred, average='macro')\n","            f1s.append(f1)\n","            accs.append(accuracy)\n","\n","    if perform_pca:\n","        evr_stats = get_stats(evrs, batch=True)\n","        ev_stats = get_stats(evs, batch=True)\n","\n","        if n_eig_view > evr_stats['mu'].size:\n","            n_eig_view = evr_stats['mu'].size\n","        print(f\"   proportion of explained variance by {n_eig_view} components: {evr_stats['mu'][:n_eig_view].sum():.2f}\")\n","        print(f\"   eigenvalues of first {n_eig_view} components: {np.round(ev_stats['mu'][:n_eig_view],2)}\\n\")\n","\n","        plot_PCA(global_file_path, evr_stats, ev_stats, vec_name, n_trials)\n","\n","    return accs, f1s\n","\n","\n","def get_stats(x, batch=False):\n","    assert type(x) is list, 'x must be a list'\n","\n","    if batch:\n","        mu = np.mean(x, axis=0)\n","        sigma = np.std(x, axis=0, ddof=1)\n","        se = sem(x)\n","        ci = [norm.interval(0.95, loc=m, scale=s) for m, s in zip(mu, se)]\n","    else:\n","        mu = np.mean(x)\n","        sigma = np.std(x, ddof=1)\n","        se = sem(x)\n","        ci = norm.interval(0.95, loc=mu, scale=se)\n","    return {'mu':mu, 'sigma': sigma, 'se': se, 'ci': ci}\n","\n","\n","def plot_classification_results(global_file_path, BERT_accs, BERT_f1s, FEAT_accs, FEAT_f1s):\n","    assert len(BERT_accs) == len(BERT_f1s) == len(FEAT_accs) == len(FEAT_f1s), 'plot lists are not same length'\n","\n","    SMALL_SIZE = 8\n","    MEDIUM_SIZE = 10\n","    BIGGER_SIZE = 12\n","\n","    plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n","    plt.rc('axes', titlesize=MEDIUM_SIZE)     # fontsize of the axes title\n","    plt.rc('axes', labelsize=SMALL_SIZE)    # fontsize of the x and y labels\n","    plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n","    plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n","    plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n","    plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n","\n","    stats = OrderedDict({'BERT accuracy': get_stats(BERT_accs),\n","                         'feature accuracy': get_stats(FEAT_accs),\n","                         'BERT f1': get_stats(BERT_f1s),\n","                         'feature f1': get_stats(FEAT_f1s)})\n","\n","    x = np.ones(len(BERT_accs))\n","\n","    fig, ax = plt.subplots()\n","\n","    # plot all trials\n","    ax.plot(x, BERT_accs, marker=',', color='#ecf469', linestyle='', label='BERT accuracies')\n","    ax.plot(x * 2, FEAT_accs, marker=',', color='#b2dd53', linestyle='', label='Feature accuracies')\n","    ax.plot(x * 3, BERT_f1s, marker=',', color='#53c14f', linestyle='', label='BERT f1s')\n","    ax.plot(x * 4, FEAT_f1s, marker=',', color='#31945a', linestyle='', label='Feature f1s')\n","\n","    # plot means and error bars\n","    for i, stat in enumerate(stats.values()):\n","        x, y = i + 1.1, stat['mu']\n","        ybot, ytop = [y - stat['ci'][0]], [stat['ci'][1] - y]\n","        ax.errorbar(x, y, yerr=(ybot, ytop), fmt='_r', ecolor='#00a6fb', label='95% CI' if i==0 else None)\n","\n","        x += 0.1\n","        ax.errorbar(x, y, yerr=stat['se'], fmt='_r', ecolor='#0582ca', label='SEM' if i==0 else None)\n","\n","        x += 0.1\n","        ax.errorbar(x, y, yerr=stat['sigma'], fmt='_r', ecolor='#006494', label='σ' if i==0 else None)\n","\n","\n","    ax.set_xticks((1.15, 2.15, 3.15, 4.15))\n","    ax.set_xticklabels(stats.keys())\n","    ax.set_xlabel('Accuracy Metric                                F1 Metric')\n","\n","    ax.set_yticks(np.linspace(0,1,11))\n","    ax.set_yticklabels(np.linspace(0,1,11))\n","    ax.set_ylabel('Scores')\n","    ax.set_title('Classification Results')\n","    ax.legend()\n","\n","    results_file = join(global_file_path, f'results_{datetime.now().strftime(\"%m-%d-%y %X\")}.png')\n","    plt.tight_layout()\n","    plt.savefig(f'{results_file}')\n","    plt.show()\n","\n","\n","def plot_PCA(global_file_path, evr_stats, ev_stats, vec_name, num_trials):\n","    SMALL_SIZE = 8\n","    MEDIUM_SIZE = 10\n","    BIGGER_SIZE = 12\n","\n","    plt.rc('font', size=SMALL_SIZE)          # controls default text sizes\n","    plt.rc('axes', titlesize=MEDIUM_SIZE)     # fontsize of the axes title\n","    plt.rc('axes', labelsize=SMALL_SIZE)    # fontsize of the x and y labels\n","    plt.rc('xtick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n","    plt.rc('ytick', labelsize=SMALL_SIZE)    # fontsize of the tick labels\n","    plt.rc('legend', fontsize=SMALL_SIZE)    # legend fontsize\n","    plt.rc('figure', titlesize=BIGGER_SIZE)  # fontsize of the figure title\n","\n","    fig, axs = plt.subplots(2, 1)\n","    num_comps = ev_stats['mu'].size\n","    x = np.arange(1, evr_stats['mu'].size + 1)\n","\n","    axs[0].plot(np.insert(x, 0, 0), np.insert(np.cumsum(evr_stats['mu']), 0, 0))\n","    axs[0].axhline(y=.8, color='r', linestyle='--')\n","\n","    x_ticks = np.linspace(0, tens_ceil(num_comps), num=11, dtype=int)\n","    axs[0].set_xticks(x_ticks)\n","    x_labels = np.char.mod('%s', x_ticks)\n","    # x_labels[0] = ''\n","    axs[0].set_xticklabels(x_labels)\n","\n","    axs[0].set_xlim(-0.5, tens_ceil(num_comps) + .5)\n","    axs[0].set_xlabel('# of Components')\n","\n","    axs[0].set_yticks(np.arange(0, 1.1, 0.1))\n","    axs[0].set_yticklabels(np.arange(0, 110, 10, dtype=int))\n","    axs[0].set_ylim(0, 1.05)\n","    axs[0].set_ylabel(f'μ % Variance Explained\\n({num_trials} trials)')\n","    axs[0].set_title('PCA Proportion of Variance Explained')\n","\n","    axs[1].bar(x, ev_stats['mu'], width=1, color='#31945a')\n","    axs[1].set_xticks(x_ticks)\n","    axs[1].set_xticklabels(x_labels)\n","    axs[1].set_xlim(0, tens_ceil(num_comps) + .5)\n","    axs[1].set_xlabel('Component #')\n","    axs[1].set_ylabel(f'μ Total Variance Explained\\n({num_trials} trials)')\n","\n","    axs[1].set_title('PCA Scree')\n","\n","    fig.suptitle(f'{vec_name} PCA Results')\n","\n","    results_file = join(global_file_path, f'pca_{datetime.now().strftime(\"%m-%d-%y %X\")}.png')\n","    plt.tight_layout()\n","    plt.savefig(f'{results_file}')\n","    plt.show()\n","\n","\n","def tens_ceil(x):\n","    return int(np.ceil(x / 10.0) * 10)\n"],"metadata":{"id":"mtB7VhQrjQvW","executionInfo":{"status":"ok","timestamp":1698116738963,"user_tz":240,"elapsed":828,"user":{"displayName":"Nat Swinger","userId":"02225308103184052469"}}},"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wAjikuX8UZnX"},"source":["# Main Run Cell"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"EjuYSNzTT-A_","outputId":"dcdfef8f-4593-4f5d-a2f2-9fe7c410d1ca"},"outputs":[{"output_type":"stream","name":"stdout","text":["Beginning data loading...\n","\n","Data loading complete, run time: 2.29 s...\n","   data has shape [19479 x 350]\n","\n","Loading pretrained BERT embeddings...\n","\n","Pretrained BERT embeddings loaded, run time: 2.57 s...\n","   embedding set has shape [19479 x 768]\n","\n","Beginning feature processing...\n","\n","Feature processing complete, run time: 0.05 s...\n","   features have shape [19479 x 346]\n","\n","Beginning classification...\n","\n"]}],"source":["global_file_path = 'drive/MyDrive/Colab Notebooks' # directory location\n","subreddit = 'c' # picks which subreddits are loaded from full data set, options: 'c', 'mh', 'nmh', 'all'\n","subsample_size = 453 # number of posts from each subreddit, note: int for balanced, float for unbalanced\n","max_batch_size=5 # number of posts per batch when tokenizing and modeling\n","max_tokens=512 # max number of tokens per post\n","pca_only = False # won't do any classification when False,\n","num_trials = 100 # number of random trials used by classify for significance testing\n","perform_pca_bert=False # performs PCA on BERT embeddings when True, uses n_pca_components, note: auto set to True if pca_only is True\n","perform_pca_feat=False # performs PCA on feature embeddings when True, uses n_pca_components, note: auto set to True if pca_only is True\n","n_pca_components_bert=None # determines the number of components for PCA dimensionality reduction for BERT, note: if None or if >= original # of dimensions, no reduction\n","n_pca_components_feat=None # determines the number of components for PCA dimensionality reduction for features, note: if None or if >= original # of dimensions, no reduction\n","n_eig_view=20 # determines number of viewed eigenvalues in printed output, no bearing on reduction: note: if > n_pca_components, defaults to n_pca_components\n","np.random.seed(0) # set random seed for subsampling df\n","\n","print(f'Beginning data loading...\\n')\n","start = time.time()\n","reddit_data = load_data(global_file_path, subsample_size=subsample_size, subreddit=subreddit)\n","stop = time.time()\n","print(f'Data loading complete, run time: {stop-start:.2f} s...\\n   data has shape [{reddit_data.shape[0]} x {reddit_data.shape[1]}]\\n')\n","\n","posts = reddit_data.post.values.tolist()\n","\n","print(f'Loading pretrained BERT embeddings...\\n')\n","psychBERT_embeddings = load_BERT_embeddings(global_file_path, posts, max_batch_size=max_batch_size, max_tokens=max_tokens, subreddit=subreddit)\n","\n","X_bert = psychBERT_embeddings\n","\n","print(f'Beginning feature processing...\\n')\n","start = time.time()\n","features = list(reddit_data.columns)\n","features = [n for n in features if n not in ['subreddit', 'author', 'date', 'post']]\n","X_feat = reddit_data[features].values\n","stop = time.time()\n","print(f'Feature processing complete, run time: {stop-start:.2f} s...\\n   features have shape [{len(X_feat)} x {len(X_feat[0])}]\\n')\n","\n","y = reddit_data.subreddit.values\n","\n","if not pca_only:\n","    print(f'Beginning classification...\\n')\n","\n","start_b = time.time()\n","accs_bert, f1s_bert = classify(global_file_path, X_bert, y, n_trials=num_trials, perform_pca=perform_pca_bert, n_pca_components=n_pca_components_bert, vec_name='BERT', n_eig_view=n_eig_view, pca_only=pca_only)\n","stop_b = time.time()\n","\n","start_f = time.time()\n","accs_feat, f1s_feat = classify(global_file_path, X_feat, y, n_trials=num_trials, perform_pca=perform_pca_feat, n_pca_components=n_pca_components_feat, vec_name='Features', n_eig_view=n_eig_view, pca_only=pca_only)\n","stop_f = time.time()\n","\n","if not pca_only:\n","    acc_bert = get_stats(accs_bert)\n","    f1_bert = get_stats(f1s_bert)\n","    acc_feat = get_stats(accs_feat)\n","    f1_feat = get_stats(f1s_feat)\n","\n","    print(f'Classification results over {num_trials} trials:')\n","    print(f\"    BERT:    Runtime: {stop_b-start_b:.2f} s    ---    mean accuracy: {acc_bert['mu']:.2f}, σ: {acc_bert['sigma']:.2f}    ---    mean f1: {f1_bert['mu']:.2f}, σ: {f1_bert['sigma']:.2f}\")\n","    print(f\"    Features:    Runtime: {stop_f-start_f:.2f} s    ---    mean accuracy: {acc_feat['mu']:.2f}, σ: {acc_feat['sigma']:.2f}    ---    mean f1: {f1_feat['mu']:.2f}, σ: {f1_feat['sigma']:.2f}\")\n","\n","    plot_classification_results(global_file_path, accs_bert, f1s_bert, accs_feat, f1s_feat)"]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4","machine_shape":"hm","mount_file_id":"1Z4KYJW73UaBBrEpVksJJzr7apW7nIP45","authorship_tag":"ABX9TyN2d+rovPTsEklVnVFF0H4v"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}